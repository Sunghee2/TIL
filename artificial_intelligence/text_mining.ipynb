{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "text_train의 타입: <class 'list'>\n",
      "text_train의 길이: 25000\n",
      "text_train[6]: \n",
      "b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n",
      "y_train[6]: \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(type(reviews_train))\n",
    "print(\"text_train의 타입: {}\".format(type(text_train)))\n",
    "print(\"text_train의 길이: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]: \\n{}\".format(text_train[6]))\n",
    "print(\"y_train[6]: \\n{}\".format(y_train[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text data 중에서 개행문자 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"The Movie was sub-par, but this Television Pilot delivers a great springboard into what has become a Sci-Fi fans Ideal program. The Actors deliver and the special effects (for a television series) are spectacular. Having an intelligent interesting script doesn't hurt either.<br /><br />Stargate SG1 is currently one of my favorite programs.\" \n",
      "\n",
      "<class 'bytes'> \n",
      "\n",
      "b\"The Movie was sub-par, but this Television Pilot delivers a great springboard into what has become a Sci-Fi fans Ideal program. The Actors deliver and the special effects (for a television series) are spectacular. Having an intelligent interesting script doesn't hurt either.  Stargate SG1 is currently one of my favorite programs.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_train[5], \"\\n\")\n",
    "print(type(text_train[5]), \"\\n\")\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "print(text_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class 별 샘플 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (훈련 데이터): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"클래스별 샘플 수 (훈련 데이터): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터의 문서 수: 25000\n",
      "테스트 데이터의 문서 수: 25000\n",
      "클래스별 샘플 수 (학습 데이터): [12500 12500]\n",
      "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "reviews_test = load_files(\"./data/aclImdb/test\")\n",
    "\n",
    "# data는 text파일, target은 폴더 번호\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "# data에서 개행문자 삭제\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "\n",
    "print(\"학습 데이터의 문서 수: {}\".format(len(text_train)))\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (학습 데이터): {}\".format(np.bincount(y_train)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어휘사전 생성 및 확인\n",
    "> CountVactorizer() : 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 13\n",
      "어휘 사전의 내용: \n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer 객체 생성\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# 문자열 리스트를 토큰으로 분리하고, 어휘사전 구축\n",
    "vect.fit(bards_words)\n",
    "\n",
    "# 어휘사전은 vocabulary_ 속성에 저장됨\n",
    "print(\"어휘 사전의 크기: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"어휘 사전의 내용: \\n{}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format> \n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1 \n",
      "\n",
      "BOW의 밀집 표현: \n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "\n",
    "# 희소행렬로 표현(0이 많기 때문)\n",
    "# repr : 숫자 -> 문자열\n",
    "print(\"BOW: {}\".format(repr(bag_of_words)), \"\\n\")\n",
    "print(bag_of_words, \"\\n\")\n",
    "\n",
    "# numpy로 변환해야 볼 수 있음. 각 document의 bow 표현\n",
    "print(\"BOW의 밀집 표현: \\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 리뷰에 대한 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# BOW 생성\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수: 74849\n",
      "처음 20개 특성:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n"
     ]
    }
   ],
   "source": [
    "# get_feature_names : 단어(특징)을 추출하여 리스트 구성\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"특성 개수: {}\".format(len(feature_names)))\n",
    "print(\"처음 20개 특성:\\n{}\".format(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20010에서 20030까지 특성:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "매 2000번째 특성:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "print(\"20010에서 20030까지 특성:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"매 2000번째 특성:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수: 318\n",
      "불용어 일부:\n",
      "['among', 'cry', 'though', 'top', 'through', 'whereas', 'be', 'con', 'can', 'eight', 'if', 'keep', 'ltd', 'sixty', 'beyond', 'bottom', 'twenty', 'everything', 'seem', 'very']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"불용어 개수: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"불용어 일부:\\n{}\".format(list(ENGLISH_STOP_WORDS)[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어가 제거된 X_train:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# stop words=\"english\"라고 지정하면 내장된 불용어를 사용함\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"불용어가 제거된 X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term:['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]] \n",
      "\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vect1 = CountVectorizer().fit(corpus)\n",
    "tf = vect1.transform(corpus)\n",
    "\n",
    "feature_names = vect1.get_feature_names()\n",
    "print(\"Term:{}\".format(feature_names[:]))\n",
    "print(tf.toarray(), \"\\n\")\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(corpus)\n",
    "tfidf = vect2.transform(corpus)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어의 쌍으로 구성하는 n-gram 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 13\n",
      "어휘 사전:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "변환된 데이터 (밀집 배열):\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\n",
    "    'The fool doth think he is wise,',\n",
    "    'but the wise man knows himself to be a fool'\n",
    "]\n",
    "\n",
    "# (1, 1) 최소, 최대 단어수\n",
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 14\n",
      "어휘 사전:\n",
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n",
      "변환된 데이터 (밀집 배열):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 두 쌍의 단어로 구성\n",
    "cv = CountVectorizer(ngram_range=(2,2)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 39\n",
      "어휘 사전:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n",
      "변환된 데이터 (밀집 배열):\n",
      "[[0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0\n",
      "  1 0 0]\n",
      " [1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1\n",
      "  1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 1-gram부터 trigram까지 모두 포함\n",
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "# 단어별 stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stemmer 객체 생성\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for stem in example_words:\n",
    "    print(ps.stem(stem)) # stem 함수로 단어별 stemming 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunghee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize 에러 해결 부분\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.']\n",
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# 문장 전체를 stemming\n",
    "new_text = \"It is important to be very pythonly while you are pythoning with python.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "print(words)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'import', 'to', 'be', 'veri', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python', '.']\n"
     ]
    }
   ],
   "source": [
    "# 화면에 출력하지 않고 단어리스트로 유지\n",
    "result = [ps.stem(w) for w in words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemming 기능을 추가한 BOW 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'like', 'swim', 'swimmer', 'thi']\n",
      "[[1 1 1 1 0]]\n",
      "[[2 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# CountVectorizer에 토큰을 생성하는 별도의 함수를 지정하고 함수 내에서 토큰화와 stemming을 실행\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "vect.fit(['This swimmer likes swimming.'])\n",
    "\n",
    "sentence1 = vect.transform(['The swimmer likes swimming.'])\n",
    "sentence2 = vect.transform(['The swimmer swim. .'])\n",
    "\n",
    "print(vect.get_feature_names())\n",
    "print(sentence1.toarray())\n",
    "print(sentence2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수: 25000\n",
      "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n",
      "X_train:\n",
      "<25000x89234 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2516865 stored elements in Compressed Sparse Row format>\n",
      "최상의 크로스 밸리데이션 점수: 0.88\n",
      "최적의 매개변수:  {'C': 0.1}\n",
      "테스트 점수: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "reviews_test = load_files('./data/aclImdb/test')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words='english', token_pattern=u\"(?u)\\b\\w\\w+\\b\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
    "\n",
    "param_grid = {'C':[0.01, 0.1]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수: \", grid.best_params_)\n",
    "\n",
    "\n",
    "X_test = vect.transform(text_test)\n",
    "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
